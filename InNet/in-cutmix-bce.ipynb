{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as tfms\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import cv2 \n",
    "import torch\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import timm\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "# base = \"/kaggle/input/hpa-single-small-dataset/\"\n",
    "\n",
    "TRAIN_DF_PATH = base + \"train.csv\"\n",
    "TRAIN_IMG_PATH = base + \"train\"\n",
    "TEST_IMG_PATH = base + \"test\"\n",
    "SAMPLE_SUB = base + \"sample_submission.csv\"\n",
    "CELL_LABEL = {\n",
    "0:  \"Nucleoplasm\", \n",
    "1:  \"Nuclear membrane\",   \n",
    "2:  \"Nucleoli\",   \n",
    "3:  \"Nucleoli fibrillar center\" ,  \n",
    "4:  \"Nuclear speckles\",\n",
    "5:  \"Nuclear bodies\",\n",
    "6:  \"Endoplasmic reticulum\",   \n",
    "7:  \"Golgi apparatus\",\n",
    "8:  \"Intermediate filaments\",\n",
    "9:  \"Actin filaments\", \n",
    "10: \"Microtubules\",\n",
    "11:  \"Mitotic spindle\",\n",
    "12:  \"Centrosome\",   \n",
    "13:  \"Plasma membrane\",\n",
    "14:  \"Mitochondria\",   \n",
    "15:  \"Aggresome\",\n",
    "16:  \"Cytosol\",   \n",
    "17:  \"Vesicles and punctate cytosolic patterns\",   \n",
    "18:  \"Negative\"\n",
    "}\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DF_PATH)\n",
    "train_df['label_count'] = train_df['Label'].apply(lambda x: len(x.split(\"|\")))\n",
    "# train_df.head()\n",
    "\n",
    "#hyperparameters\n",
    "CLASS = 19\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "RESIZE = 256\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() \\\n",
    "         else torch.device('cpu')\n",
    "PATH = base\n",
    "TRAIN_DIR = PATH + 'train/'\n",
    "TEST_DIR = PATH + 'test/'\n",
    "\n",
    "#imagenet transform\n",
    "img_tfms = tfms.Compose([\n",
    "    tfms.ToPILImage(),\n",
    "    tfms.RandomHorizontalFlip(),\n",
    "    tfms.RandomVerticalFlip(),\n",
    "    tfms.RandomRotation(20),\n",
    "    tfms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "DEVICE\n",
    "\n",
    "class HPADataset(Dataset):\n",
    "    def __init__(self, csv_path, ids, labels=None, resize=None, transforms=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.ids = ids\n",
    "        self.labels = labels  # Can be None for test set\n",
    "        self.resize = resize\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        img_path = os.path.join(self.csv_path, img_id + '_green.png')\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        if self.resize:\n",
    "            image = cv2.resize(image, (self.resize, self.resize))\n",
    "\n",
    "        # Ensure image is uint8 (for transforms compatibility)\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        if self.labels is not None:  # Training/Validation mode\n",
    "            label = self.labels[index]\n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "        else:  # Test mode\n",
    "            return image, img_id\n",
    "#model\n",
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self,output_features, model_name = 'inception_v3', pertrained=True):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pertrained)\n",
    "        # InceptionNetV3 uses .fc for the classifier\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(nn.Linear(in_features, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(512, output_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class CNNet(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(CNNet, self).__init__()\n",
    "        self.model = torchvision.models.resnet34(pretrained=True)\n",
    "        self.model.fc = nn.Sequential(nn.Linear(input_features, 100),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(100, output_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # BCE with logits\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        pt = torch.where(targets == 1, probas, 1 - probas)\n",
    "        focal_term = self.alpha * (1 - pt) ** self.gamma\n",
    "        loss = focal_term * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class CutMixDataset(Dataset):\n",
    "    def __init__(self, dataset, num_classes=19, beta=1.0, prob=0.5):\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "        self.beta = beta\n",
    "        self.prob = prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image1, label1 = self.dataset[idx]\n",
    "\n",
    "        # Apply CutMix with some probability\n",
    "        if random.random() < self.prob:\n",
    "            # Select random second sample\n",
    "            idx2 = random.randint(0, len(self.dataset) - 1)\n",
    "            image2, label2 = self.dataset[idx2]\n",
    "\n",
    "            lam = np.random.beta(self.beta, self.beta)\n",
    "            bbx1, bby1, bbx2, bby2 = self.rand_bbox(image1.size()[1:], lam)\n",
    "            image1[:, bby1:bby2, bbx1:bbx2] = image2[:, bby1:bby2, bbx1:bbx2]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (image1.size()[-1] * image1.size()[-2]))\n",
    "            label = label1 * lam + label2 * (1. - lam)\n",
    "            return image1, label\n",
    "\n",
    "        else:\n",
    "            return image1, label1\n",
    "\n",
    "    def rand_bbox(self, size, lam):\n",
    "        W = size[0]\n",
    "        H = size[1]\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "\n",
    "        # uniform\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "        return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "print(\"cell run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = InceptionNet(CLASS)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_df = pd.read_csv(PATH + \"train.csv\")\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Split string labels into list of ints\n",
    "train_df['Label'] = train_df['Label'].apply(lambda x: list(map(int, x.split(\"|\"))))\n",
    "\n",
    "# Fit binarizer\n",
    "mlb = MultiLabelBinarizer(classes=range(CLASS))  # Ensure consistent order\n",
    "y_bin = mlb.fit_transform(train_df['Label'])\n",
    "\n",
    "# Then pass y_bin to your dataset\n",
    "X_train = train_df['ID'].values\n",
    "X_ds = HPADataset(TRAIN_DIR, X_train, y_bin, RESIZE, img_tfms)\n",
    "\n",
    "# 80-20 split\n",
    "total_size = len(X_ds)\n",
    "val_size = int(0.2 * total_size)\n",
    "train_size = total_size - val_size\n",
    "train_ds, valid_ds = random_split(X_ds, [train_size, val_size])\n",
    "\n",
    "\n",
    "cutmix_train_ds = CutMixDataset(train_ds, num_classes=CLASS, beta=1.0, prob=0.5)\n",
    "train_dl = DataLoader(cutmix_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "from sklearn.metrics import f1_score, average_precision_score, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, valid_dl, threshold=0.5, device=DEVICE):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = [], []\n",
    "    images_sample, preds_sample, labels_sample = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, lbl in valid_dl:\n",
    "            img = img.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "            out = torch.sigmoid(model(img.float()))\n",
    "\n",
    "            all_preds.append(out.cpu().numpy())\n",
    "            all_trues.append(lbl.cpu().numpy())\n",
    "\n",
    "            if len(images_sample) < 8:  # Save samples for plotting\n",
    "                images_sample.extend(img.cpu().numpy())\n",
    "                preds_sample.extend(out.cpu().numpy())\n",
    "                labels_sample.extend(lbl.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds)\n",
    "    trues = np.concatenate(all_trues)\n",
    "\n",
    "    preds_bin = (preds > threshold).astype(int)\n",
    "\n",
    "    f1 = f1_score(trues, preds_bin, average='macro')\n",
    "    map_score = average_precision_score(trues, preds, average='macro')\n",
    "    conf_matrix = multilabel_confusion_matrix(trues, preds_bin)\n",
    "\n",
    "    print(f\"F1 Score: {f1:.4f}\", end = \"\\t\")\n",
    "    print(f\"mAP: {map_score:.4f}\")\n",
    "\n",
    "print(\"cell done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train loop\n",
    "loss_hist = []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    losses = []\n",
    "    model = model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_dl):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = model(image.float())\n",
    "        loss = loss_fn(output, label)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_hist.append(sum(losses)/len(losses))\n",
    "    \n",
    "    print(f\"epoch: {epoch} loss:{sum(losses)/len(losses):.4f}\", end=\"\\t\")\n",
    "\n",
    "    evaluate_model(model, valid_dl)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dir, test_csv_path=None, threshold=0.5, device=DEVICE, \n",
    "               batch_size=BATCH_SIZE, resize=RESIZE, save_predictions=False, output_path=None):\n",
    "    \"\"\"Test the trained model on the test dataset.\"\"\"\n",
    "    test_tfms = tfms.Compose([\n",
    "        tfms.ToPILImage(),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    has_labels = False\n",
    "    results = None\n",
    "    \n",
    "    if test_csv_path and os.path.exists(test_csv_path):\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "        test_ids = test_df['ID'].values\n",
    "        \n",
    "\n",
    "        if 'Label' in test_df.columns:\n",
    "            test_df['Label'] = test_df['Label'].apply(lambda x: list(map(int, x.split(\"|\"))))\n",
    "            mlb = MultiLabelBinarizer(classes=range(CLASS))\n",
    "            test_labels = mlb.fit_transform(test_df['Label'])\n",
    "            test_ds = HPADataset(test_dir, test_ids, test_labels, resize, test_tfms)\n",
    "            test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            print(f\"Testing on {len(test_ds)} samples with labels...\")\n",
    "            model.eval()\n",
    "            all_preds, all_trues = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for img, lbl in tqdm(test_dl):\n",
    "                    img = img.to(device)\n",
    "                    lbl = lbl.to(device)\n",
    "                    out = torch.sigmoid(model(img.float()))\n",
    "                    all_preds.append(out.cpu().numpy())\n",
    "                    all_trues.append(lbl.cpu().numpy())\n",
    "            \n",
    "            predictions = np.concatenate(all_preds)\n",
    "            true_labels = np.concatenate(all_trues)\n",
    "            preds_bin = (predictions > threshold).astype(int)\n",
    "            \n",
    "            f1 = f1_score(true_labels, preds_bin, average='macro')\n",
    "            map_score = average_precision_score(true_labels, predictions, average='macro')\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Test Results:\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"Test F1 Score (macro): {f1:.4f}\")\n",
    "            print(f\"Test mAP (macro): {map_score:.4f}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "            \n",
    "            results = {\n",
    "                'f1_score': f1,\n",
    "                'map_score': map_score,\n",
    "                'predictions': predictions,\n",
    "                'true_labels': true_labels,\n",
    "                'preds_binary': preds_bin,\n",
    "                'image_ids': test_ids\n",
    "            }\n",
    "            has_labels = True\n",
    "        else:\n",
    "\n",
    "            test_ds = HPADataset(test_dir, test_ids, None, resize, test_tfms)\n",
    "            test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "\n",
    "        if os.path.exists(SAMPLE_SUB):\n",
    "            test_df = pd.read_csv(SAMPLE_SUB)\n",
    "            test_ids = test_df['ID'].values\n",
    "        else:\n",
    "\n",
    "            test_files = [f.replace('_green.png', '') for f in os.listdir(test_dir) \n",
    "                         if f.endswith('_green.png')]\n",
    "            test_ids = np.unique(test_files)\n",
    "        \n",
    "        test_ds = HPADataset(test_dir, test_ids, None, resize, test_tfms)\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    if not has_labels:\n",
    "        print(f\"Testing on {len(test_ds)} samples...\")\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img, img_id in tqdm(test_dl):\n",
    "                img = img.to(device)\n",
    "                out = torch.sigmoid(model(img.float()))\n",
    "                all_preds.append(out.cpu().numpy())\n",
    "                all_ids.extend(img_id)\n",
    "        \n",
    "        predictions = np.concatenate(all_preds)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Test Inference Complete\")\n",
    "        print(f\"Processed {len(predictions)} samples\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        results = {\n",
    "            'predictions': predictions,\n",
    "            'image_ids': all_ids if all_ids else test_ids\n",
    "        }\n",
    "    \n",
    "    if save_predictions:\n",
    "        if output_path is None:\n",
    "            output_path = 'test_predictions.csv'\n",
    "        \n",
    "\n",
    "        pred_df = pd.DataFrame(predictions, columns=[f'class_{i}' for i in range(CLASS)])\n",
    "        pred_df.insert(0, 'ID', results.get('image_ids', test_ids))\n",
    "        \n",
    "\n",
    "        def get_labels(row):\n",
    "            pred_classes = np.where(row[1:] > threshold)[0]\n",
    "            return '|'.join(map(str, pred_classes)) if len(pred_classes) > 0 else '18'  # 18 is 'Negative'\n",
    "        \n",
    "        submission_df = pd.DataFrame({'ID': pred_df['ID']})\n",
    "        submission_df['Label'] = pred_df.apply(get_labels, axis=1)\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"Predictions saved to {output_path}\")\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
